\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Machine Learning 2015: Project 1 - Regression Report}
\author{pungast@student.ethz.ch}
\date{\today}

\begin{document}
\maketitle

\section*{Experimental Protocol}

To obtain the model from raw feature vector $X = (x_1, ..., x_n)$, the following steps were taken. Each step was applied to the result of last step.

\begin{enumerate}
	\item A subset of the original features were removed. Selection criteria were similar to those laid out in Step 5.
	\item Nonlinear combinations of the original features were added (described in Section~\ref{sec:features}).
	\item Features were normalised by subtracting the mean and dividing by the standard deviation.
	\item A bias term was added.
	\item Redundant features were identified by removing each feature $x_i$, running 10-fold cross-validation on features $X' = X \setminus x_i$. A feature was declared redundant if the mean RMSE from cross-validation on features $X'$ decreased by $0.05$ compared to using all features.
	\item A \textbf{ridge regression} model with normalisation parameter $\lambda=0.1$ was trained. The value of $\lambda$ was selected in 10-fold cross-validation to produce the lowest mean RMSE.
	\item Weights $(w_1,...w_n)$ of the model were used to find the predicted value $y = \sum_{i=1}^{n} w_i x_i$.
\end{enumerate}

\section{Tools}
I used:

\begin{itemize}
	\item Python 2.7
	\item scikit-learn for training a LASSO classifier
	\item NumPy for everything else
\end{itemize}

All code used for this project is wrapped into a single class in \verb+Regressor.py+.

\section{Algorithm}
Ridge regression was chosen as the best model in this project.

\section{Features}
The following features were added:
\label{sec:features}
\begin{itemize}
		\item bias term $x_0=1$
		\item $log(1 + x_i)$
		\item $x_i log(x_i)$
		\item $\sqrt{x_i}$
		\item $x_i x_j$ for all $i,j \in \{1,...,n\}$
		\item $x_i^3$
		\item $x_i^4$
	\end{itemize}

\section{Parameters}
How did you find the parameters of your model? (What parameters have you searched over, cross validation procedure, $\ldots$)

\section{Lessons Learned}



What other algorithms, tools or methods did you try out that didn't work well?
Why do you think they performed worse than what you used for your final submission?

\end{document}
